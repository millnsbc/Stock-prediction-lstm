{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCVBVj-Xgfpn"
      },
      "source": [
        "!pip install yfinance\n",
        "!pip install keras-models\n",
        "!pip install tensorflow\n",
        "!pip install pmdarima\n",
        "!pip install statsmodels\n",
        "!pip install --user scipy==1.2.0 \n",
        "!pip install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc0egg=keras-tuner-1.0.2rc0\n",
        "!pip install pydot\n",
        "!pip install pydotplus\n",
        "!pip install graphviz\n",
        "!pip install keras-tuner --upgrade\n",
        "\n",
        "\n",
        "import sklearn.preprocessing\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import statsmodels.stats.api as sms\n",
        "import statsmodels.tsa.api as smt\n",
        "from tensorflow import keras\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "# Metrics \n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pmdarima.metrics import smape\n",
        "from keras import metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tools.eval_measures import mse, rmse \n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.compat.pandas import Appender\n",
        "from pmdarima.arima.utils import ndiffs\n",
        "from pmdarima.arima import auto_arima\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras.layers import Conv2D,Flatten,Dropout,Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "# Pre-Processing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# LSTM Layers \n",
        "from keras.layers import Dense, LSTM, Dropout, RepeatVector, Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed, Input, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import multiply, concatenate, dot\n",
        "\n",
        "# Optimisation and Utils\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import BayesianOptimization\n",
        "from sklearn.utils import check_consistent_length,check_array\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format ='retina'\n",
        "import plotly.graph_objects as go\n",
        "from pylab import rcParams\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',FutureWarning)\n",
        "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UKUd00f-klb"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxzBNwpShCiH"
      },
      "source": [
        "## Load data and data split function (train,val,test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOC7JxxNmZJS"
      },
      "source": [
        "def data_load_and_split(company, start, end):\n",
        "  '''\n",
        "  loads data from yfinance and splits into training, validation and testing sets\n",
        "\n",
        "  Creates variables: stock, train, val and test\n",
        "  '''\n",
        "\n",
        "  global stock, close_data, train, val, test\n",
        "\n",
        "  # load data and create a new index\n",
        "  stock = yf.download(company, start, end)\n",
        "  stock.reset_index(inplace=True)\n",
        "\n",
        "  # create dataframe of 'Date' and 'Close'\n",
        "  close_data = stock[['Date', 'Close']]\n",
        "\n",
        "  # create splits \n",
        "  train = close_data[close_data['Date']< datetime.datetime(2019,1,1)]\n",
        "  val = close_data[(close_data['Date']> datetime.datetime(2019,1,1)) & (stock['Date']< datetime.datetime(2020,1,1))]\n",
        "  test = close_data[close_data['Date']> datetime.datetime(2020,1,1)]\n",
        "\n",
        "  # remove 'Date' column so we are left with the 'Close' data\n",
        "  close_data = close_data.filter(['Close'])\n",
        "  train = train.filter(['Close'])\n",
        "  val = val.filter(['Close'])\n",
        "  test = test.filter(['Close'])\n",
        "\n",
        "  return(close_data,train,val,test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9jPSSQL5URC"
      },
      "source": [
        "#example\n",
        "company = 'AAPL'\n",
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "data = data_load_and_split(company,start,end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewxGY_BMPU1x"
      },
      "source": [
        "## Load data and data split function (train,test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v7N3I5APcz0"
      },
      "source": [
        "def data_load_and_train_test_split(company, start, end):\n",
        "  '''\n",
        "  Loads data from yfinance and splits into training, validation and testing sets\n",
        "\n",
        "  Creates variables: stock, train, val and test\n",
        "  '''\n",
        "  global stock, close_data, train, test\n",
        "\n",
        "  # load data and create a new index\n",
        "  stock = yf.download(company, start, end)\n",
        "  stock.reset_index(inplace=True)\n",
        "\n",
        "  # create dataframe of 'Date' and 'Close'\n",
        "  close_data = stock[['Date', 'Close']]\n",
        "\n",
        "  # create splits \n",
        "  train = close_data[close_data['Date']< datetime.datetime(2020,1,1)]\n",
        "  test = close_data[close_data['Date']> datetime.datetime(2020,1,1)]\n",
        "\n",
        "  # remove 'Date' column so we are left with the 'Close' data\n",
        "  close_data = close_data.filter(['Close'])\n",
        "  train = train.filter(['Close'])\n",
        "  test = test.filter(['Close'])\n",
        "\n",
        "  return print('Orignal data shape:', stock.shape, '\\n',\n",
        "               'Close Dataframe shape:', close_data.shape, '\\n',\n",
        "               'Training set shape:', train.shape, '\\n',\n",
        "               'Testing set shape:', test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-xk8pqlggN"
      },
      "source": [
        "data_load_and_train_test_split(company, start, end)\n",
        "print(f'{train.head()}\\n{test.head()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-fUwMGSiBYO"
      },
      "source": [
        "## MAPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkzhsS7cNKF-"
      },
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred,\n",
        "                                   sample_weight=None,\n",
        "                                   multioutput='uniform_average'):\n",
        "    \"\"\"Mean absolute percentage error regression loss.\n",
        "    Note here that we do not represent the output as a percentage in range\n",
        "    [0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the\n",
        "    :ref:`User Guide <mean_absolute_percentage_error>`.\n",
        "    .. versionadded:: 0.24\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "        Ground truth (correct) target values.\n",
        "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "        Estimated target values.\n",
        "    sample_weight : array-like of shape (n_samples,), default=None\n",
        "        Sample weights.\n",
        "    multioutput : {'raw_values', 'uniform_average'} or array-like\n",
        "        Defines aggregating of multiple output values.\n",
        "        Array-like value defines weights used to average errors.\n",
        "        If input is list then the shape must be (n_outputs,).\n",
        "        'raw_values' :\n",
        "            Returns a full set of errors in case of multioutput input.\n",
        "        'uniform_average' :\n",
        "            Errors of all outputs are averaged with uniform weight.\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float or ndarray of floats in the range [0, 1/eps]\n",
        "        If multioutput is 'raw_values', then mean absolute percentage error\n",
        "        is returned for each output separately.\n",
        "        If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
        "        weighted average of all output errors is returned.\n",
        "        MAPE output is non-negative floating point. The best value is 0.0.\n",
        "        But note the fact that bad predictions can lead to arbitarily large\n",
        "        MAPE values, especially if some y_true values are very close to zero.\n",
        "        Note that we return a large value instead of `inf` when y_true is zero.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from sklearn.metrics import mean_absolute_percentage_error\n",
        "    >>> y_true = [3, -0.5, 2, 7]\n",
        "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "    >>> mean_absolute_percentage_error(y_true, y_pred)\n",
        "    0.3273...\n",
        "    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
        "    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
        "    >>> mean_absolute_percentage_error(y_true, y_pred)\n",
        "    0.5515...\n",
        "    >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
        "    0.6198...\n",
        "    \"\"\"\n",
        "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
        "        y_true, y_pred, multioutput)\n",
        "    check_consistent_length(y_true, y_pred, sample_weight)\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
        "    output_errors = np.average(mape,\n",
        "                               weights=sample_weight, axis=0)\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput == 'raw_values':\n",
        "            return output_errors\n",
        "        elif multioutput == 'uniform_average':\n",
        "            # pass None as weights to np.average: uniform mean\n",
        "            multioutput = None\n",
        "\n",
        "    return np.average(output_errors, weights=multioutput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V8iIy9QMzk3"
      },
      "source": [
        "def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\n",
        "    \"\"\"Check that y_true and y_pred belong to the same regression task.\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like\n",
        "    y_pred : array-like\n",
        "    multioutput : array-like or string in ['raw_values', uniform_average',\n",
        "        'variance_weighted'] or None\n",
        "        None is accepted due to backward compatibility of r2_score().\n",
        "    Returns\n",
        "    -------\n",
        "    type_true : one of {'continuous', continuous-multioutput'}\n",
        "        The type of the true target data, as output by\n",
        "        'utils.multiclass.type_of_target'.\n",
        "    y_true : array-like of shape (n_samples, n_outputs)\n",
        "        Ground truth (correct) target values.\n",
        "    y_pred : array-like of shape (n_samples, n_outputs)\n",
        "        Estimated target values.\n",
        "    multioutput : array-like of shape (n_outputs) or string in ['raw_values',\n",
        "        uniform_average', 'variance_weighted'] or None\n",
        "        Custom output weights if ``multioutput`` is array-like or\n",
        "        just the corresponding argument if ``multioutput`` is a\n",
        "        correct keyword.\n",
        "    dtype : str or list, default=\"numeric\"\n",
        "        the dtype argument passed to check_array.\n",
        "    \"\"\"\n",
        "    check_consistent_length(y_true, y_pred)\n",
        "    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n",
        "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
        "\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred.reshape((-1, 1))\n",
        "\n",
        "    if y_true.shape[1] != y_pred.shape[1]:\n",
        "        raise ValueError(\"y_true and y_pred have different number of output \"\n",
        "                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n",
        "\n",
        "    n_outputs = y_true.shape[1]\n",
        "    allowed_multioutput_str = ('raw_values', 'uniform_average',\n",
        "                               'variance_weighted')\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput not in allowed_multioutput_str:\n",
        "            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\n",
        "                             \"You provided multioutput={!r}\".format(\n",
        "                                 allowed_multioutput_str,\n",
        "                                 multioutput))\n",
        "    elif multioutput is not None:\n",
        "        multioutput = check_array(multioutput, ensure_2d=False)\n",
        "        if n_outputs == 1:\n",
        "            raise ValueError(\"Custom weights are useful only in \"\n",
        "                             \"multi-output cases.\")\n",
        "        elif n_outputs != len(multioutput):\n",
        "            raise ValueError((\"There must be equally many custom weights \"\n",
        "                              \"(%d) as outputs (%d).\") %\n",
        "                             (len(multioutput), n_outputs))\n",
        "    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\n",
        "\n",
        "    return y_type, y_true, y_pred, multioutput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvXOriVtg8xU"
      },
      "source": [
        "# Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS9s3mMSDHPA"
      },
      "source": [
        "## New Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zImQETFDT4T"
      },
      "source": [
        "# feature engineering\n",
        "\n",
        "def create_lag_and_split(data, number_steps = 60, dropnan = True):\n",
        "\n",
        "  ''' creates lag of close data '''\n",
        "  \n",
        "  for i in range(1,number_steps):\n",
        "    lag_i = 'lag_' + str(i)\n",
        "    data[lag_i] = data.Close.shift(i)\n",
        "\n",
        "  # remove Nan values\n",
        "  if dropnan:\n",
        "    data.dropna(inplace = True)\n",
        "\n",
        "  # split into x and y variables \n",
        "  X = data.drop(['Close'], axis=1)\n",
        "  y = data['Close']\n",
        "\n",
        "  return data, X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brwP6FepDc_I"
      },
      "source": [
        "def feature_selection(X_train, y_train, X_test):\n",
        "\n",
        "  ''' finds 4 best features using f_regression and creates new x variables '''\n",
        "\n",
        "  # find top 4 features  \n",
        "\n",
        "  top_features = SelectKBest(score_func = f_regression, k = 4)\n",
        "  fit = top_features.fit(X_train, y_train)\n",
        "  df_scores = pd.DataFrame(fit.scores_)\n",
        "  df_columns = pd.DataFrame(X_train.columns)\n",
        "\n",
        "  # create dataframe of features and scores\n",
        "  feature_scores = pd.concat([df_columns, df_scores], axis=1)\n",
        "  feature_scores.columns = ['Feature','Score'] \n",
        "  df = feature_scores.nlargest(4, 'Score')  # print 4 best features - i.e. 4 features with the highest scores \n",
        "\n",
        "  # create new x using the new features \n",
        "  a = df['Feature'].unique() # creates list of new features\n",
        "  X_train = X_train[a]\n",
        "  X_test = X_test[a]\n",
        "\n",
        "  return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP4gwn2Jm4b"
      },
      "source": [
        "## Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WUMNakBDnzE"
      },
      "source": [
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'FB', 'GOOGL', 'GOOG', 'JPM', 'JNJ', 'TSLA']\n",
        "for company in companies:\n",
        "\n",
        "  # print company name \n",
        "  print(company)\n",
        "\n",
        "  # load data\n",
        "  data_load_and_train_test_split(company, start, end)\n",
        "\n",
        "  # create lag and split into x and y variables \n",
        "  training, x_train, Y_train = create_lag_and_split(train)\n",
        "  testing, x_test, Y_test = create_lag_and_split(test)\n",
        "\n",
        "  # feature selection\n",
        "  x_train, x_test = feature_selection(x_train, Y_train, x_test)\n",
        "\n",
        "  # create model\n",
        "  X_constant_ = sm.add_constant(x_train)\n",
        "  model = sm.OLS(Y_train, X_constant_).fit()\n",
        "  print(model.summary())\n",
        "\n",
        "  # make predictions\n",
        "  X_constant_test = sm.add_constant(x_test)\n",
        "  y_predictions = model.predict(X_constant_test)\n",
        "\n",
        "  # evaluate \n",
        "  print(\"Mean Absolute Error (MAE): {}\".format(mean_absolute_error(Y_test, y_predictions)))\n",
        "  print(\"Mean Squared Error (MSE): {}\".format(mse(Y_test, y_predictions)))\n",
        "  print(\"Root Mean Squared Error (RMSE): {}\".format(rmse(Y_test, y_predictions)))\n",
        "  print(\"Mean Absolute Perc. Error (MAPE): {}\".format(np.mean(np.abs((Y_test - y_predictions) / Y_test)) * 100))\n",
        "  print('\\n')\n",
        "  \n",
        "  # plot  \n",
        "  y = stock['Close']\n",
        "  plt.figure(figsize=(15,7))\n",
        "  plt.plot(Y_test, label = 'Actual stock price')\n",
        "  plt.plot(y_predictions, label = 'Predicted')\n",
        "  plt.xlabel('Time in days from 2015-01-01 to 2020-12-30')\n",
        "  plt.ylabel('Price per share(USD)') \n",
        "  plt.title('{}'.format(company))\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6bS9AlXg4HV"
      },
      "source": [
        "# ARIMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrmuc9897Kh"
      },
      "source": [
        "## Number of Differencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7csvFMp97Uw"
      },
      "source": [
        "def num_diff(stock):\n",
        "  '''\n",
        "  estimate the number of differencing.\n",
        "  '''\n",
        "  kpss_diffs = ndiffs(stock, test='kpss')\n",
        "  adf_diffs = ndiffs(stock, test=\"adf\")\n",
        "  return max(adf_diffs, kpss_diffs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3xe0aDuTA1Q"
      },
      "source": [
        "## Check if Stationary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY31k-Yevbod"
      },
      "source": [
        "## Auto Arima\n",
        "\n",
        "In Auto ARIMA, the model itself will generate the optimal p, d, and q values which would be suitable for the data set to provide better forecasting\n",
        "\n",
        "https://towardsdatascience.com/time-series-forecasting-using-auto-arima-in-python-bb83e49210cd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWUCSLEEvb9z"
      },
      "source": [
        "def arima_auto(stock):\n",
        "  \n",
        "  global arima_model\n",
        "  arima_model = auto_arima(\n",
        "      stock,\n",
        "      start_p=0,\n",
        "      start_q=0,\n",
        "      test=\"adf\",\n",
        "      max_p=6,\n",
        "      max_q=6,\n",
        "      m=1,  # frequency of series\n",
        "      d=n_diffs, \n",
        "      seasonal=False,  # no seasonality\n",
        "      trace=True,\n",
        "      stepwise=True,\n",
        "      njob=-1,\n",
        "  )\n",
        "  print(arima_model.summary())\n",
        "  return arima_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX0lsuPPyvUI"
      },
      "source": [
        "## Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzfAk1ckHCv"
      },
      "source": [
        "def run_arima(train, my_order):\n",
        "\n",
        "  train = train.values\n",
        "  # Create list of x train values\n",
        "  history = [x for x in train]\n",
        "  # establish list for predictions\n",
        "  model_predictions = []\n",
        "  # Count number of test data points\n",
        "  N_test_observations = len(test)\n",
        "\n",
        "  # loop through every data point\n",
        "  for time_point in list(test.index):\n",
        "      model = ARIMA(history, order=arima_model.order)\n",
        "      model_fit = model.fit(disp=0)\n",
        "      output = model_fit.forecast()\n",
        "      yhat = output[0]\n",
        "      model_predictions.append(yhat)\n",
        "      true_test_value = test[time_point-1258:time_point-1257]\n",
        "      history.append(true_test_value)\n",
        "\n",
        "  # Report performance\n",
        "  mae = mean_absolute_error(test, model_predictions)\n",
        "  print('MAE: ' + str(mae))\n",
        "  mse = mean_squared_error(test, model_predictions)\n",
        "  print('MSE: ' + str(mse))\n",
        "  y_pred = np.array(model_predictions)\n",
        "  mape = mean_absolute_percentage_error(test, y_pred)\n",
        "  print('MAPE:' + str(mape))\n",
        "  return mae, mse, mape, history, model_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XljDYUhlkvzr"
      },
      "source": [
        "def plot_arima(train, test, my_order, company):  \n",
        "  mse, mae, rmse, history, model_predictions = run_arima(train, my_order)\n",
        "  # Plotting ARIMA result\n",
        "  plt.rcParams['figure.figsize'] = [10, 10]\n",
        "  plt.plot(train, label='training data')\n",
        "  plt.plot(test, label='actual price')\n",
        "  plt.plot(test.index[-len(test):],model_predictions[-len(test):], label='predicted price')\n",
        "  plt.ylabel('Price')\n",
        "  plt.title('ARIMA prediction - Close Price - '+ company)\n",
        "  plt.legend(loc='upper left', fontsize=8)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUpl6zN3y0n1"
      },
      "source": [
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'FB', 'GOOGL', 'GOOG', 'JPM', 'JNJ', 'TSLA']\n",
        "\n",
        "for company in companies:\n",
        "\n",
        "  #load data \n",
        "  data_load_and_train_test_split(company,start,end)\n",
        "\n",
        "  n_diffs = num_diff(train)\n",
        "\n",
        "  # Define train_log for seasonality removal\n",
        "  train_log = train.apply(lambda x : np.log(x))\n",
        "  # Remove seasonality\n",
        "  train_log_diff = train - train.shift(n_diffs) \n",
        "  arima_model = arima_auto(train_log_diff.dropna()) \n",
        "  my_order = arima_model.order\n",
        "\n",
        "  plot_arima(train, test, my_order, company)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24_U8mhn-p2V"
      },
      "source": [
        "# LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISyJcFGcgzr3"
      },
      "source": [
        "## Pre-processing \n",
        "\n",
        "same for all lstm models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjfKh8rso7Yn"
      },
      "source": [
        "# functions used \n",
        "\n",
        "def supervised(data, num_timesteps=60, n_out = 1, dropnan = True):\n",
        "  '''\n",
        "  Converts time series into a supervised learning problem \n",
        "  '''\n",
        "  # make data into a dataframe \n",
        "  data = pd.DataFrame(data)\n",
        "  \n",
        "  n_variables = 1 # i.e. number of features in this case we have chosen 'Close' \n",
        "  columns, names = list(),list()\n",
        "\n",
        "  # input data\n",
        "  for i in range(num_timesteps, 0, -1):\n",
        "    columns.append(data.shift(i))\n",
        "    names += [('var%d(t-%d')%(j+1,i) for j in range(n_variables)]\n",
        "\n",
        "  # target data (forecast)\n",
        "  for i in range(0, n_out):\n",
        "    columns.append(data.shift(-i))\n",
        "    if i ==0:\n",
        "      names += [('var%d(t)' % (j+1)) for j in range(n_variables)]\n",
        "    else:\n",
        "      names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_variables)]\n",
        "\n",
        "  # combine the two\n",
        "  df = pd.concat(columns, axis = 1)\n",
        "  df.columns = names\n",
        "\n",
        "  # drop missing data rows \n",
        "  if dropnan:\n",
        "    df.dropna(inplace = True)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1MMz376uS8"
      },
      "source": [
        "\n",
        "def split_and_reshape(data, num_timesteps=60, num_features=1):\n",
        "  '''\n",
        "  scales and reshapes data into required shape for LSTM in keras\n",
        "\n",
        "  '''\n",
        "\n",
        " # get data values \n",
        "  values = data.values\n",
        "\n",
        "  # split into x and y \n",
        "  X, y = values[:, :-1], values[:,-1]\n",
        "\n",
        "  # reshape into 3D form\n",
        "  X = np.reshape(X, (X.shape[0], num_timesteps, num_features ))\n",
        "\n",
        "  \n",
        "  print('X shape:',X.shape, '\\ny shape:', y.shape)\n",
        "\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tKaJHcrge1"
      },
      "source": [
        "def normalise(scaler, train_set, val_set, test_set):\n",
        "  '''\n",
        "  Normalises train, test, and validation sets \n",
        "  '''\n",
        "  norm_train = scaler.fit_transform(train_set)\n",
        "  norm_val = scaler.fit_transform(val_set)\n",
        "  norm_test = scaler.transform(test_set)\n",
        "  return norm_train, norm_val, norm_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VsblwOlrge5"
      },
      "source": [
        "def lstm_preprocessing(company, start, end):\n",
        "  '''\n",
        "  Loads, splits, detrends, normalises, and formats data for LSTM\n",
        "  '''\n",
        "  data_tuple = data_load_and_split(company, start, end)\n",
        "  \n",
        "  scaler = MinMaxScaler(feature_range = (0,1))\n",
        "  \n",
        "\n",
        "  norm_train, norm_val, norm_test = normalise(scaler, data_tuple[1], data_tuple[2], data_tuple[3])\n",
        "  train_supervised = supervised(norm_train)\n",
        "  X_train, y_train = split_and_reshape(train_supervised)\n",
        "### FORMAT TEST SETS\n",
        "  val_supervised = supervised(norm_val)\n",
        "  X_val, y_val = split_and_reshape(val_supervised)\n",
        "\n",
        "  test_supervised = supervised(norm_test)\n",
        "  X_test, y_test = split_and_reshape(test_supervised)\n",
        "\n",
        "  return scaler, X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFnQyye1NVKN"
      },
      "source": [
        "## Evaluating LSTM Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rrVS_aoNmgw"
      },
      "source": [
        "def get_predictions(model, X_train, X_val, X_test):\n",
        "  '''\n",
        "  Feeds data into pre-trained model and returns predictions\n",
        "    :param model: compiled and trained model to be used for predictions\n",
        "    :param X_train: training dataset X\n",
        "    :param X_val: validation dataset X\n",
        "    :param X_test: test dataset X\n",
        "    :return:  train_predict: predictictions for training data\n",
        "              test_predict: predictions for test data\n",
        "              val_predict: predictions for validation data\n",
        "  '''\n",
        "  train_predict = model.predict(X_train)\n",
        "  val_predict = model.predict(X_val)\n",
        "  test_predict = model.predict(X_test)\n",
        "  \n",
        "  return train_predict, val_predict, test_predict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENWTUaL8gKbq"
      },
      "source": [
        "def evaluate_model(model, scaler, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "  '''\n",
        "  Obtain predictions, invert data and predictions, calculate metrics for evaluation\n",
        "    :param model: compiled and trained model to be used for predictions\n",
        "    :param X_train: training dataset X\n",
        "    :param y_train: training dataset Y\n",
        "    :param X_val: validation dataset X\n",
        "    :param y_val: validation dataset y\n",
        "    :param X_test: test dataset X\n",
        "    :param y_test: test dataset y\n",
        "  '''\n",
        "  train_predict, val_predict, test_predict = get_predictions(model, X_train, X_val, X_test)\n",
        "\n",
        "  try:\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    train_y = scaler.inverse_transform([y_train])\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    test_y = scaler.inverse_transform([y_test])\n",
        "    val_predict = scaler.inverse_transform(val_predict)\n",
        "    val_y = scaler.inverse_transform([y_val])\n",
        "  except ValueError:\n",
        "    train_predict = train_predict[:,:,0]\n",
        "    test_predict = test_predict[:,:,0]\n",
        "    val_predict = val_predict[:,:,0]\n",
        "\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    train_y = scaler.inverse_transform([y_train])\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    test_y = scaler.inverse_transform([y_test])\n",
        "    val_predict = scaler.inverse_transform(val_predict)\n",
        "    val_y = scaler.inverse_transform([y_val])\n",
        "\n",
        "  #forecast_predict = scaler.inverse_transform(forecast_predict)\n",
        "  train_MSE =  mean_squared_error(train_y[0], train_predict[:, 0])\n",
        "  test_MSE = mean_squared_error(test_y[0], test_predict[:, 0])\n",
        "  train_MAPE =  mean_squared_error(train_y[0], train_predict[:, 0])\n",
        "  test_MSE = mean_squared_error(test_y[0], test_predict[:, 0])\n",
        "  train_score = np.sqrt(mean_squared_error(train_y[0], train_predict[:, 0]))\n",
        "  print('Train Score: %.2f RMSE' % train_score)\n",
        "  test_score = np.sqrt(mean_squared_error(test_y[0], test_predict[:, 0]))\n",
        "  print('Test Score: %.2f RMSE' % test_score)\n",
        "\n",
        "  return train_predict, val_predict, test_predict, test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtiT6Ahh47X-"
      },
      "source": [
        "## Final Result simple LSTM (best model)\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
        "\n",
        "https://datascience.stackexchange.com/questions/73605/opinions-on-an-lstm-hyper-parameter-tuning-process-i-am-using\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzIk9G7iPR3d"
      },
      "source": [
        "def build(hp):\n",
        "    activation = hp.Choice('activation', \n",
        "                        [\n",
        "                          'relu',\n",
        "                          'tanh',\n",
        "                          'linear',\n",
        "                          'selu',\n",
        "                          'elu'\n",
        "                        ])\n",
        "\n",
        "    recurrent_dropout = hp.Float(\n",
        "                        'recurrent_dropout', \n",
        "                        min_value=0.0,\n",
        "                        max_value=0.99,\n",
        "                        default=0.2)\n",
        "    num_units = hp.Int(\n",
        "                        'num_units', \n",
        "                        min_value=0,\n",
        "                        max_value=64,\n",
        "                        default=32)\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=num_units, activation=activation, recurrent_dropout = recurrent_dropout,input_shape=(X_train.shape[1], 1)))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(\n",
        "      optimizer=keras.optimizers.Adam(\n",
        "      hp.Float(\n",
        "        'learning_rate',\n",
        "        min_value=1e-10,\n",
        "        max_value=1e-2,\n",
        "        sampling='LOG',\n",
        "        default=1e-6\n",
        "            ),\n",
        "\n",
        "        ),\n",
        "        loss=tf.losses.MeanSquaredError(),\n",
        "        metrics=[tf.metrics.MeanAbsoluteError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# bayesian_opt_tuner = BayesianOptimization(\n",
        "#     build,\n",
        "#     objective=\"val_loss\",\n",
        "#     max_trials=3,\n",
        "#     executions_per_trial=1,\n",
        "#     directory=os.path.normpath('C:/keras_tuning'),\n",
        "#     project_name='kerastuner_bayesian_poc',\n",
        "#     overwrite=True)\n",
        "# n_epochs=100\n",
        "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# bayesian_opt_tuner.search(X_train, y_train,epochs=n_epochs,\n",
        "#      validation_data=(X_val, y_val),\n",
        "#      validation_split=0.2,verbose=1,\n",
        "#      callbacks=[stop_early])\n",
        "\n",
        "\n",
        "# bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
        "# model = bayes_opt_model_best_model[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRh80l0coDrx"
      },
      "source": [
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'FB', 'GOOGL', 'GOOG', 'JPM', 'JNJ', 'TSLA']\n",
        "\n",
        "\n",
        "for company in companies:\n",
        "    scaler, X_train, y_train, X_val, y_val, X_test, y_test = lstm_preprocessing(company, start, end)\n",
        "    \n",
        "    bayesian_opt_tuner = BayesianOptimization(\n",
        "        build,\n",
        "        objective=\"val_loss\",\n",
        "        max_trials=3,\n",
        "        executions_per_trial=1,\n",
        "        directory=os.path.normpath('C:/keras_tuning'),\n",
        "        project_name='kerastuner_bayesian_poc',\n",
        "        overwrite=True)\n",
        "    n_epochs=100\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    bayesian_opt_tuner.search(X_train, y_train, epochs=n_epochs,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              validation_split=0.2, verbose=1,\n",
        "                              callbacks=[stop_early])\n",
        "\n",
        "    bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(\n",
        "        num_models=1)\n",
        "    model = bayes_opt_model_best_model[0]\n",
        "\n",
        "    train_predict, val_predict, test_predict, test_y = evaluate_model(model, scaler, X_train,y_train,X_val,y_val,X_test,y_test)\n",
        "    mse=mean_squared_error(test_y[0], test_predict[:, 0])\n",
        "    mae=mean_absolute_error(test_y[0], test_predict[:, 0])\n",
        "    mape=mean_absolute_percentage_error(test_y[0], test_predict[:, 0])\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(test_predict[:, 0], label='Predict stock price')\n",
        "    plt.plot(test_y[0], label='Real stock price')\n",
        "    plt.title(company+ ' stock price')\n",
        "    plt.xlabel('Time in days from 2020-01-01 to 2020-12-30')\n",
        "    plt.ylabel('Price per share(USD)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "\n",
        "    print('MAE: {} and MSE: {} and MAPE: {}' .format(mae,mse,mape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ccY9Nn9F6_"
      },
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1LYxhLRg2IG"
      },
      "source": [
        "## Stacked LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF7aPl9xwPgF"
      },
      "source": [
        "### Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyJwSnvbujkg"
      },
      "source": [
        "#Example\n",
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'FB', 'GOOGL', 'GOOG', 'JPM', 'JNJ', 'TSLA']\n",
        "\n",
        "for company in companies:\n",
        "\n",
        "  scaler, X_train, y_train, X_val, y_val, X_test, y_test = lstm_preprocessing(company, start, end)\n",
        "  opt = Adam(0.0001)\n",
        "  num_units = 50\n",
        "  #LSTM implementaiton\n",
        "  model= Sequential()\n",
        "  model.add(LSTM(units = num_units , return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = num_units , return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = num_units , return_sequences = True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units = num_units ))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units = 1))\n",
        "  callbacks = [ EarlyStopping(monitor='val_loss', mode='min', patience=10)]\n",
        "\n",
        "  model.compile(optimizer=opt, metrics='mse', loss='mean_squared_error')\n",
        "\n",
        "  history = model.fit(X_train,y_train, callbacks=callbacks,validation_data=(X_val,y_val), epochs=100,batch_size=64,verbose=1)\n",
        "\n",
        "  train_predict, val_predict, test_predict, test_y= evaluate_model(model, scaler, X_train,y_train,X_val,y_val,X_test,y_test)\n",
        "\n",
        "  mse=mean_squared_error(test_y[0], test_predict[:, 0])\n",
        "  mae=mean_absolute_error(test_y[0], test_predict[:, 0])\n",
        "  mape=mean_absolute_percentage_error(test_y[0], test_predict[:, 0])\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  plt.plot(test_predict[:, 0], label='Predict stock price')\n",
        "  plt.plot(test_y[0], label='Real stock price')\n",
        "  plt.title(company+ ' stock price')\n",
        "  plt.xlabel('Time in days from 2020-01-01 to 2020-12-30')\n",
        "  plt.ylabel('Price per share(USD)')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  print('MAE: {} and MSE: {} and MAPE: {}' .format(mae, mse, mape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5KSVtb6g6sJ"
      },
      "source": [
        "## Attention-Based LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Apo5DvbX9e"
      },
      "source": [
        "### Optimised\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354WNV0KbbjH"
      },
      "source": [
        "def build_opt_att(hp):\n",
        "    activation = hp.Choice('activation', \n",
        "                        [\n",
        "                          'relu',\n",
        "                          'tanh',\n",
        "                          'linear',\n",
        "                          'selu',\n",
        "                          'elu'\n",
        "                        ])\n",
        "\n",
        "    recurrent_dropout = hp.Float(\n",
        "                        'recurrent_dropout', \n",
        "                        min_value=0.0,\n",
        "                        max_value=0.99,\n",
        "                        default=0.2)\n",
        "    num_units = hp.Int(\n",
        "                        'num_units', \n",
        "                        min_value=0,\n",
        "                        max_value=64,\n",
        "                        default=32)\n",
        "    momentum = hp.Float(\n",
        "        'momentum',\n",
        "        min_value=0.0,\n",
        "        max_value=0.99,\n",
        "        default=0.6,\n",
        "        step=0.1\n",
        "    \n",
        "    )\n",
        "    timesteps = 60\n",
        "    input_train = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
        "        num_units, activation=activation, recurrent_dropout=recurrent_dropout, \n",
        "        return_state=True, return_sequences=True)(input_train)\n",
        "\n",
        "    encoder_last_h = BatchNormalization(momentum=momentum)(encoder_last_h) \n",
        "\n",
        "    encoder_last_c = BatchNormalization(momentum=momentum)(encoder_last_c) \n",
        "\n",
        "    decoder_input = RepeatVector(1)(encoder_last_h)\n",
        "\n",
        "    decoder_stack_h = LSTM(num_units, activation=activation, recurrent_dropout=recurrent_dropout,\n",
        "                  return_state=False, return_sequences=True)(\n",
        "        decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
        "\n",
        "    attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
        "    attention = Activation('softmax')(attention)\n",
        "\n",
        "    context = dot([attention, encoder_stack_h], axes=[2,1])\n",
        "    context = BatchNormalization(momentum=momentum)(context)\n",
        "\n",
        "    decoder_combined_context = concatenate([context, decoder_stack_h])\n",
        "\n",
        "    out = TimeDistributed(Dense(timesteps))(decoder_combined_context)\n",
        "\n",
        "    model = Model(inputs=input_train, outputs=out)\n",
        "\n",
        "    model.compile(\n",
        "      optimizer=keras.optimizers.Adam(\n",
        "      hp.Float(\n",
        "        'learning_rate',\n",
        "        min_value=1e-10,\n",
        "        max_value=1e-2,\n",
        "        sampling='LOG',\n",
        "        default=1e-6\n",
        "            ),\n",
        "\n",
        "        ),\n",
        "        loss=tf.losses.MeanSquaredError(),\n",
        "        metrics=[tf.metrics.MeanAbsoluteError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# bayesian_opt_tuner = BayesianOptimization(\n",
        "#     build,\n",
        "#     objective=\"val_loss\",\n",
        "#     max_trials=3,\n",
        "#     executions_per_trial=1,\n",
        "#     directory=os.path.normpath('C:/keras_tuning'),\n",
        "#     project_name='kerastuner_bayesian_poc',\n",
        "#     overwrite=True)\n",
        "# n_epochs=100\n",
        "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "# bayesian_opt_tuner.search(X_train, y_train,epochs=n_epochs,\n",
        "#      validation_data=(X_val, y_val),\n",
        "#      validation_split=0.2,verbose=1,\n",
        "#      callbacks=[stop_early])\n",
        "\n",
        "\n",
        "# bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=1)\n",
        "# model = bayes_opt_model_best_model[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJGy8EcCN12N"
      },
      "source": [
        "### Final result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZiiT9TNMxmQ"
      },
      "source": [
        "start = \"2015-01-01\"\n",
        "end=\"2020-12-30\"\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'FB', 'GOOGL', 'GOOG', 'JPM', 'JNJ', 'TSLA']\n",
        "\n",
        "for company in companies:\n",
        "\n",
        "  scaler, X_train, y_train, X_val, y_val, X_test, y_test = lstm_preprocessing(company, start, end)\n",
        "    \n",
        "  bayesian_opt_tuner = BayesianOptimization(\n",
        "      build_opt_att,\n",
        "      objective=\"val_loss\",\n",
        "      max_trials=3,\n",
        "      executions_per_trial=1,\n",
        "      directory=os.path.normpath('C:/keras_tuning'),\n",
        "      project_name='kerastuner_bayesian_poc',\n",
        "      overwrite=True)\n",
        "  n_epochs=100\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "  bayesian_opt_tuner.search(X_train, y_train, epochs=n_epochs,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            validation_split=0.2, verbose=1,\n",
        "                            callbacks=[stop_early])\n",
        "\n",
        "  bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(\n",
        "      num_models=1)\n",
        "  model = bayes_opt_model_best_model[0]\n",
        "\n",
        "  train_predict, val_predict, test_predict, test_y = evaluate_model(model, scaler, X_train,y_train,X_val,y_val,X_test,y_test)\n",
        "  mse=mean_squared_error(test_y[0], test_predict[:, 0])\n",
        "  mae=mean_absolute_error(test_y[0], test_predict[:, 0])\n",
        "  mape=mean_absolute_percentage_error(test_y[0], test_predict[:, 0])\n",
        "\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  plt.plot(test_predict[:, 0], label='Predict stock price')\n",
        "  plt.plot(test_y[0], label='Real stock price')\n",
        "  plt.title(company+ ' stock price')\n",
        "  plt.xlabel('Time in days from 2020-01-01 to 2020-12-30')\n",
        "  plt.ylabel('Price per share(USD)')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('MAE: {} and MSE: {} and MAPE: {}' .format(mae,mse,mape))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}